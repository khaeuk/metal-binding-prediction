{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OneHot + ProtVec BRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T07:18:36.460244Z",
     "start_time": "2018-03-08T07:18:36.020509Z"
    },
    "collapsed": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('Metal_all_20180116.snappy.parquet')\n",
    "# print ('***** Data Types *****' + '\\n' + str(df.dtypes) + '\\n\\n' + \n",
    "#        '***** Unique Ligands *****' + '\\n' + str(df.ligandId.unique()))\n",
    "\n",
    "# Extract zinc-binded, single-chained protein sequences\n",
    "df_zn = df.loc[df['ligandId'] == 'ZN']\n",
    "df_zn_single = df_zn.loc[df_zn['interactingChains'] == 1]\n",
    "seqs = np.array(df_zn_single.sequence)\n",
    "target = np.array(df_zn_single.fingerprint)\n",
    "\n",
    "del df,df_zn,df_zn_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T07:18:36.560963Z",
     "start_time": "2018-03-08T07:18:36.539906Z"
    },
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length is 22784\n"
     ]
    }
   ],
   "source": [
    "# Remove seqs containing 'U' and 'X'\n",
    "rows_to_delete = []\n",
    "for i in range(seqs.shape[0]):\n",
    "    if 'X' in seqs[i] or 'U' in seqs[i]:\n",
    "#         print('Removing...' + str(i))\n",
    "        rows_to_delete.append(i)        \n",
    "        \n",
    "seqs = np.delete(seqs, rows_to_delete, 0)\n",
    "target = np.delete(target, rows_to_delete)\n",
    "print (\"Sequence length is \" + str(seqs.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol Dict:  {'D': 9266, 'S': 185, 'H': 29210, 'C': 37542, 'E': 4625, 'K': 831, 'G': 47, 'Y': 75, 'N': 217, 'Q': 48, 'A': 31, 'W': 44, 'T': 147, 'R': 32, 'M': 21, 'F': 14, 'I': 18, 'V': 11, 'L': 24, 'P': 2}\n",
      "Total:  82390\n",
      "[C,H / Total]:  0.8101954120645709\n"
     ]
    }
   ],
   "source": [
    "symbol_counts = {}\n",
    "for i in range(seqs.shape[0]):\n",
    "    for j in range(target[i].shape[0]):\n",
    "        if seqs[i][target[i][j]] not in symbol_counts.keys():\n",
    "            symbol_counts[seqs[i][target[i][j]]] = 1\n",
    "        else:\n",
    "            symbol_counts[seqs[i][target[i][j]]] += 1\n",
    "\n",
    "print ('Symbol Dict: ', symbol_counts)\n",
    "print ('Total: ', sum(symbol_counts.values()))\n",
    "print ('[C,H / Total]: ', (symbol_counts['C'] + symbol_counts['H']) / sum(symbol_counts.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "#### Proportion of C,H binding sites \n",
    "\n",
    "All seqs: 0.37\n",
    "\n",
    "Zn-binded, single-chain: 0.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T07:18:37.647341Z",
     "start_time": "2018-03-08T07:18:37.643354Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "dict_path = \"./dictionaries/\"\n",
    "model_path = \"./models/\"\n",
    "hist_path = \"./histories/\"\n",
    "fig_path = \"./figs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Input - ProtVec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T07:18:38.487881Z",
     "start_time": "2018-03-08T07:18:38.038546Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load\n",
    "seqs_dict = {}\n",
    "with open(dict_path + \"seq_n_gram_to_vec_dict_w_UX\", 'r') as fp:\n",
    "        seqs_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input - Onehot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "onehot_keys = []\n",
    "for i in range(seqs.shape[0]):\n",
    "    for j in range(len(seqs[i])):\n",
    "        if seqs[i][j] not in onehot_keys:\n",
    "            onehot_keys.append(seqs[i][j])\n",
    "# import operator\n",
    "# print (sorted(seqs_dict_onehot.items(), key=operator.itemgetter(1)))\n",
    "onehot_keys = np.array(onehot_keys).reshape(-1)\n",
    "np.random.shuffle(onehot_keys)\n",
    "\n",
    "seqs_dict_onehot = {}\n",
    "onehot_matrix = np.eye(onehot_keys.shape[0])\n",
    "for i in range(onehot_keys.shape[0]):\n",
    "    seqs_dict_onehot[onehot_keys[i]] = onehot_matrix[i].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T07:18:41.549232Z",
     "start_time": "2018-03-08T07:18:41.545223Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "seqs_dict_onehot = {}\n",
    "with open(dict_path + \"seqs_dict_onehot\", 'r') as fp:\n",
    "        seqs_dict_onehot = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T07:18:44.905636Z",
     "start_time": "2018-03-08T07:18:44.756795Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Onehot encoding given a cutoff (Padded with 0s / Replicate partial sequence at the end)\n",
    "def to_onehot(target_single, cutoff, seq_size, replicate=False):\n",
    "    t = [0] * cutoff\n",
    "    offset = cutoff - seq_size\n",
    "    for i in target_single:\n",
    "        t[i] = 1 \n",
    "        if replicate and (i >= seq_size-offset) and (i < seq_size):\n",
    "            t[offset+i] = 1\n",
    "    return t\n",
    "\n",
    "# Convert n-gram to vector given a dictionary\n",
    "def to_vector(seqs_dict, seq, index, n_gram_size, dimension):\n",
    "    if (index+n_gram_size) < len(seq):\n",
    "        word = seq[index: index+n_gram_size]\n",
    "        if '0' not in word:\n",
    "            if word in seqs_dict.keys():\n",
    "                return seqs_dict[word]\n",
    "    return [0] * dimension\n",
    "\n",
    "# Discard sequences longer than the cutoff and reconstruct datasets\n",
    "def optimal_cutoff(seqs, target, cutoff, replicate=False):\n",
    "    new_seqs = []\n",
    "    new_target = []\n",
    "    new_lengths = []\n",
    "    for i, s, t in zip(range(seqs.shape[0]), seqs, target):\n",
    "        if len(s) <= cutoff:\n",
    "            if replicate:\n",
    "                offset = cutoff-len(s)\n",
    "                s = s + s[len(s)-offset : len(s)]\n",
    "                new_seqs.append(s)\n",
    "            else:\n",
    "                new_seqs.append(s + (cutoff-len(s)) * '0')\n",
    "            new_target.append(to_onehot(t,cutoff,len(s), replicate))\n",
    "            new_lengths.append(len(s))\n",
    "    return new_seqs, new_target, new_lengths\n",
    "\n",
    "# Construct input tensor (Num_Samples x Cutoff X Dimension)\n",
    "def construct_X(seqs_dict, seqs, cutoff, n_gram_size, dimension, sample_size, start_index):\n",
    "    X = np.zeros((sample_size, cutoff, dimension))\n",
    "    for i in range(sample_size):\n",
    "        for j in range(cutoff):\n",
    "            X[i][j] = np.array(to_vector(seqs_dict, seqs[start_index + i], j, n_gram_size, dimension))\n",
    "    return X\n",
    "\n",
    "# Get a pair of inputs for testing\n",
    "def randomSample(X, Y):\n",
    "    size = X.shape[0]\n",
    "#     print (X.shape)\n",
    "#     print (Y.shape)\n",
    "    r = np.random.randint(size)\n",
    "    retX = X[r].reshape((1, X.shape[1], X.shape[2]))\n",
    "    retY = Y[r].reshape((1, Y.shape[1]))\n",
    "    return (retX, retY)\n",
    "\n",
    "# Paired randomization \n",
    "def shuffleXY(X, Y):\n",
    "    C = list(zip(X, Y))\n",
    "    random.shuffle(C)\n",
    "    X_ret, Y_ret = zip(*C)\n",
    "    return X_ret, Y_ret\n",
    "\n",
    "# For hybrid model only\n",
    "def random_sample_n_times(X, Y, size):\n",
    "    X1, X2 = X\n",
    "    n = X1.shape[0]\n",
    "    _X1, _X2, _Y = [], [], []\n",
    "    for i in range(size):\n",
    "        r = np.random.randint(n)\n",
    "        _X1.append(X1[r].tolist())\n",
    "        _X2.append(X2[r].tolist())\n",
    "        _Y.append(Y[r].tolist())\n",
    "        \n",
    "    _X = [np.asarray(_X1), np.asarray(_X2)]\n",
    "    _Y = np.asarray(_Y)\n",
    "    \n",
    "    return _X, _Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T07:18:48.238066Z",
     "start_time": "2018-03-08T07:18:48.047471Z"
    },
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff is 706\n",
      "Training set size is 19577\n",
      "Test set size is 2176\n"
     ]
    }
   ],
   "source": [
    "lengths = np.zeros(len(seqs))\n",
    "for i in range(len(seqs)):\n",
    "    lengths[i] = len(seqs[i])\n",
    "\n",
    "cutoff = int(np.mean(lengths) + 2 * np.std(lengths))\n",
    "print ('Cutoff is ' + str(cutoff))\n",
    "cv_ratio = 0.9\n",
    "\n",
    "X, T, L = optimal_cutoff(seqs, target, cutoff)\n",
    "X_train, X_val = X[:(int)(len(X) * cv_ratio)], X[(int)(len(X) * cv_ratio):]\n",
    "T_train, T_val = T[:(int)(len(T) * cv_ratio)], T[(int)(len(T) * cv_ratio):]\n",
    "print ('Training set size is ' + str(len(X_train)))\n",
    "print ('Test set size is ' + str(len(X_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T07:19:00.465612Z",
     "start_time": "2018-03-08T07:19:00.461601Z"
    },
    "collapsed": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "seqs_dicts = [seqs_dict_onehot, seqs_dict]\n",
    "dimensions = [20, 100]\n",
    "lstm_size = 64\n",
    "n_gram_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T07:19:16.957769Z",
     "start_time": "2018-03-08T07:19:15.811437Z"
    },
    "hide_input": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dense,Dropout, TimeDistributed, Bidirectional, Input, Concatenate, Flatten\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "# Visualization\n",
    "from keras.utils import plot_model\n",
    "# Custom layer\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T07:22:20.820484Z",
     "start_time": "2018-03-08T07:22:20.735225Z"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "class custom_callback(Callback):\n",
    "    def __init__(self,training_data,validation_data, sample_size, threshold_func):\n",
    "        self.sample_size = sample_size\n",
    "        self.threshold_func = threshold_func\n",
    "        \n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "        \n",
    "        self.roc_scores = []\n",
    "        self.f1_scores = []\n",
    "        self.precision_scores = []\n",
    "        self.recall_scores = []\n",
    "        \n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.roc_scores = []\n",
    "        self.f1_scores = []\n",
    "        self.precision_scores = []\n",
    "        self.recall_scores = []\n",
    "        return\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = random_sample_n_times(self.x, self.y, self.sample_size)\n",
    "        x_val, y_val = random_sample_n_times(self.x_val, self.y_val, self.sample_size)\n",
    "\n",
    "        y_pred = self.model.predict(x)\n",
    "        y_pred_val = self.model.predict(x_val)\n",
    "\n",
    "        roc = roc_auc_score(y.ravel(), y_pred.ravel())\n",
    "        roc_val = roc_auc_score(y_val.ravel(), y_pred_val.ravel())\n",
    " \n",
    "        y_pred = threshold_func(y_pred).astype(int)\n",
    "        y_pred_val = threshold_func(y_pred_val).astype(int)\n",
    "        \n",
    "        f1 = f1_score(y.ravel(), y_pred.ravel())\n",
    "        precision = precision_score(y.ravel(), y_pred.ravel())\n",
    "        recall = recall_score(y.ravel(), y_pred.ravel())\n",
    "        \n",
    "        f1_val = f1_score(y_val.ravel(), y_pred_val.ravel())\n",
    "        precision_val = precision_score(y_val.ravel(), y_pred_val.ravel())\n",
    "        recall_val = recall_score(y_val.ravel(), y_pred_val.ravel())\n",
    "        print (round(f1_val,4))\n",
    "#         print (roc_val, f1_val, precision_val, recall_val)\n",
    "        self.roc_scores.append((round(roc,4), round(roc_val,4)))\n",
    "        self.f1_scores.append((round(f1,4), round(f1_val,4)))\n",
    "        self.precision_scores.append((round(precision,4), round(precision_val,4)))\n",
    "        self.recall_scores.append((round(recall,4), round(recall_val,4)))\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T07:19:47.911531Z",
     "start_time": "2018-03-08T07:19:47.891478Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class aligned_dense_layer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(aligned_dense_layer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create trainable weight variables for this layer.\n",
    "        self.kernel_1 = self.add_weight(name='kernel_1', \n",
    "                                      shape=(input_shape[0][1], 1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.kernel_2 = self.add_weight(name='kernel_2', \n",
    "                                      shape=(input_shape[1][1], 1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        \n",
    "        self.kernel_offset = self.add_weight(name='kernel_offset', \n",
    "                                      shape=(input_shape[0][1], 1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        \n",
    "        super(aligned_dense_layer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.tanh((x[0] * self.kernel_1 + x[1] * self.kernel_2 + self.kernel_offset))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T07:20:04.152595Z",
     "start_time": "2018-03-08T07:20:03.121375Z"
    },
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 706, 20)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 706, 100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 706, 64)      43520       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 706, 64)      84480       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 706, 64)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 706, 64)      0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 706, 1)       65          dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 706, 1)       65          dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "aligned_dense_layer_1 (aligned_ (None, 706, 1)       2118        time_distributed_1[0][0]         \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 130,248\n",
      "Trainable params: 130,248\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = [Input(shape=(cutoff, dimensions[0])), \n",
    "          Input(shape=(cutoff, dimensions[1]))]\n",
    "towers = [Bidirectional(LSTM(lstm_size, return_sequences=True), \n",
    "                        input_shape=(cutoff,dimensions[0]), \n",
    "                        merge_mode='ave')(inputs[0]),\n",
    "          Bidirectional(LSTM(lstm_size, return_sequences=True), \n",
    "                        input_shape=(cutoff,dimensions[1]), \n",
    "                        merge_mode='ave')(inputs[1])]\n",
    "\n",
    "towers = [Dropout(0.2)(towers[0]),\n",
    "          Dropout(0.2)(towers[1])]\n",
    "\n",
    "towers = [TimeDistributed(Dense(1, activation='tanh'))(towers[0]), \n",
    "          TimeDistributed(Dense(1, activation='tanh'))(towers[1])]\n",
    "\n",
    "# towers = [Flatten()(towers[0]), \n",
    "#           Flatten()(towers[1])]\n",
    "\n",
    "\n",
    "output = aligned_dense_layer()([towers[0], towers[1]])\n",
    "# output = Flatten()(output)\n",
    "# output = Dense(1, activation='softmax')(output)\n",
    "\n",
    "adam = Adam(lr=0.001)\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figs/model.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file= fig_path + 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T07:22:37.948224Z",
     "start_time": "2018-03-08T07:22:37.943214Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def threshold_func(Y):\n",
    "    _Y = np.zeros_like(Y)\n",
    "    for i in range(Y.shape[0]):\n",
    "        t= np.mean(Y[i]) + 2 * np.std(Y[i])\n",
    "#         print(t)\n",
    "        _Y[i] = (Y[i] > t)\n",
    "    return _Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-08T08:02:58.919Z"
    },
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Epoch  1  ***\n",
      "[0] to [500]\n",
      "0.0928\n",
      "[500] to [1000]\n",
      "0.095\n",
      "[1000] to [1500]\n",
      "0.1033\n",
      "[1500] to [2000]\n",
      "0.1038\n",
      "[2000] to [2500]\n",
      "0.0783\n",
      "[2500] to [3000]\n",
      "0.0811\n",
      "[3000] to [3500]\n",
      "0.0961\n",
      "[3500] to [4000]\n",
      "0.1055\n",
      "[4000] to [4500]\n",
      "0.0952\n",
      "[4500] to [5000]\n",
      "0.0955\n",
      "[5000] to [5500]\n",
      "0.0798\n",
      "[5500] to [6000]\n",
      "0.0925\n",
      "[6000] to [6500]\n",
      "0.0823\n",
      "[6500] to [7000]\n",
      "0.0928\n",
      "[7000] to [7500]\n",
      "0.098\n",
      "[7500] to [8000]\n",
      "0.0978\n",
      "[8000] to [8500]\n",
      "0.1072\n",
      "[8500] to [9000]\n",
      "0.0916\n",
      "[9000] to [9500]\n",
      "0.0582\n",
      "[9500] to [10000]\n",
      "0.0571\n",
      "[10000] to [10500]\n",
      "0.0629\n",
      "[10500] to [11000]\n",
      "0.0609\n",
      "[11000] to [11500]\n",
      "0.0674\n",
      "[11500] to [12000]\n",
      "0.0883\n",
      "[12000] to [12500]\n",
      "0.0813\n",
      "[12500] to [13000]\n",
      "0.0859\n",
      "[13000] to [13500]\n",
      "0.096\n",
      "[13500] to [14000]\n",
      "0.0899\n",
      "[14000] to [14500]\n",
      "0.1022\n",
      "[14500] to [15000]\n",
      "0.086\n",
      "[15000] to [15500]\n",
      "0.0986\n",
      "[15500] to [16000]\n",
      "0.1061\n",
      "[16000] to [16500]\n",
      "0.0928\n",
      "[16500] to [17000]\n",
      "0.1016\n",
      "[17000] to [17500]\n"
     ]
    }
   ],
   "source": [
    "history_of_loss = [] # Accumulated loss over epoch * num_of_samples / fold_size\n",
    "history_of_roc = []\n",
    "history_of_f1 = []\n",
    "history_of_precision = []\n",
    "history_of_recall = []\n",
    "\n",
    "epoch = 1\n",
    "batch_size = 100 # Minibatch size\n",
    "fold_size = 500 # Input tensor size (fold_size, cutoff, dimension)\n",
    "\n",
    "def training_procedure():\n",
    "    \n",
    "    # Shuffle batch at each epoch\n",
    "    X_train_shuffled, T_train_shuffled = shuffleXY(X_train, T_train)\n",
    "    X_val_shuffled, T_val_shuffled = shuffleXY(X_val, T_val)\n",
    "    \n",
    "    # Construct validation sets\n",
    "    X_Vals = [construct_X(seqs_dicts[0], X_val_shuffled, cutoff, n_gram_size, dimensions[0], len(X_val_shuffled), 0),\n",
    "              construct_X(seqs_dicts[1], X_val_shuffled, cutoff, n_gram_size, dimensions[1], len(X_val_shuffled), 0)]\n",
    "    T_Vals = np.array(T_val_shuffled).reshape(len(T_val_shuffled), cutoff,1)\n",
    "    \n",
    "    # 1 to n-1 folds\n",
    "    n = int(len(X_train) / fold_size)\n",
    "    for i in range(n):\n",
    "        print('[' + str(i * fold_size) + '] to [' + str((i+1) * fold_size) + ']')   \n",
    "        X = [construct_X(seqs_dicts[0], X_train_shuffled, cutoff, n_gram_size, dimensions[0], fold_size, i * fold_size),\n",
    "             construct_X(seqs_dicts[1], X_train_shuffled, cutoff, n_gram_size, dimensions[1], fold_size, i * fold_size)]\n",
    "        \n",
    "        T = np.array(T_train_shuffled[i * fold_size: (i+1) * fold_size]).reshape(fold_size,cutoff,1)\n",
    "\n",
    "        cb = custom_callback((X,T), (X_Vals, T_Vals), batch_size, threshold_func)\n",
    "        hist = model.fit(X, T, epochs=1, batch_size=batch_size, verbose=0, callbacks=[cb])\n",
    "        \n",
    "        history_of_loss.append(hist.history['loss'])\n",
    "        history_of_roc.extend(cb.roc_scores)\n",
    "        history_of_f1.extend(cb.f1_scores)\n",
    "        history_of_precision.extend(cb.precision_scores)\n",
    "        history_of_recall.extend(cb.recall_scores)\n",
    "\n",
    "    # Last fold\n",
    "    last_fold_size = len(X_train) - n * fold_size\n",
    "    print('[' + str(n * fold_size) + '] to [' + str(n * fold_size + last_fold_size) + ']')   \n",
    "\n",
    "    X = [construct_X(seqs_dicts[0], X_train_shuffled, cutoff, n_gram_size, dimensions[0], last_fold_size, n * fold_size),\n",
    "         construct_X(seqs_dicts[1], X_train_shuffled, cutoff, n_gram_size, dimensions[1], last_fold_size, n * fold_size)]\n",
    "    T = np.array(T_train_shuffled[n * fold_size : n * fold_size + last_fold_size]).reshape(last_fold_size,cutoff,1)\n",
    "\n",
    "    cb = custom_callback((X,T), (X_Vals, T_Vals), batch_size, threshold_func)\n",
    "    hist = model.fit(X, T, epochs=1, batch_size=batch_size, verbose=0, callbacks=[cb])\n",
    "\n",
    "    history_of_loss.append(hist.history['loss'])\n",
    "    history_of_roc.extend(cb.roc_scores)\n",
    "    history_of_f1.extend(cb.f1_scores)\n",
    "    history_of_precision.extend(cb.precision_scores)\n",
    "    history_of_recall.extend(cb.recall_scores)\n",
    "    \n",
    "for i in range(epoch):\n",
    "    print ('\\n\\n*** Epoch ', i+1, ' ***')\n",
    "    training_procedure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Many to Many BRNN (Not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "class roc_callback(Callback):\n",
    "    def __init__(self,training_data,validation_data):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "        self.roc_scores = []\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.roc_scores = []\n",
    "        return\n",
    "    def on_train_end(self, logs={}):\n",
    "        return \n",
    "#     def on_epoch_begin(self, epoch, logs={}):\n",
    "#         return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.x)\n",
    "#         print (self.y.shape, y_pred.shape)\n",
    "        roc = roc_auc_score(self.y.ravel(), y_pred.ravel())\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "#         print (self.y_val.shape, y_pred_val.shape)\n",
    "\n",
    "        roc_val = roc_auc_score(self.y_val.ravel(), y_pred_val.ravel())\n",
    "#         print('\\rroc-auc: %s - roc-auc_val: %s' % (str(round(roc,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n",
    "        self.roc_scores.append((round(roc,4), round(roc_val,4)))\n",
    "        return \n",
    "\n",
    "#     def on_batch_begin(self, batch, logs={}):\n",
    "#         return\n",
    "#     def on_batch_end(self, batch, logs={}):\n",
    "#         return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "run_control": {
     "frozen": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "# Build Model\n",
    "bd_model = Sequential()\n",
    "# model.add()\n",
    "bd_model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(cutoff,dimension), merge_mode='ave'))\n",
    "bd_model.add(Dropout(0.2))\n",
    "bd_model.add(TimeDistributed(Dense(1, activation='tanh')))\n",
    "bd_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[])\n",
    "bd_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model = bd_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#Input (50,706,100) = 3,530,000 [141200000] \n",
    "#Output (50,706,1) = 35,300 [1412000]\n",
    "\n",
    "history_of_loss = [] # Accumulated loss over epoch * num_of_samples / fold_size\n",
    "histroy_of_roc = []\n",
    "epoch = 20\n",
    "batch_size = 50 # Minibatch size\n",
    "fold_size = 1000 # Input tensor size (fold_size, cutoff, dimension)\n",
    "\n",
    "def training_procedure():\n",
    "    \n",
    "    # Shuffle batch at each epoch\n",
    "    X_train_shuffled, T_train_shuffled = shuffleXY(X_train, T_train)\n",
    "    X_val_shuffled, T_val_shuffled = shuffleXY(X_val, T_val)\n",
    "    \n",
    "    # Construct validation sets\n",
    "    X_Val = construct_X(seqs_dict, X_val_shuffled, cutoff, n_gram_size, dimension, len(X_val_shuffled), 0)\n",
    "    T_Val = np.array(T_val_shuffled).reshape(len(T_val_shuffled), cutoff,1)\n",
    "    \n",
    "    # 1 to n-1 folds\n",
    "    n = int(len(X_train) / fold_size)\n",
    "    for i in range(n):\n",
    "        print('[' + str(i * fold_size) + '] to [' + str((i+1) * fold_size) + ']')   \n",
    "        X = construct_X(seqs_dict, X_train_shuffled, cutoff, n_gram_size, dimension, fold_size, i * fold_size)\n",
    "        T = np.array(T_train_shuffled[i * fold_size: (i+1) * fold_size]).reshape(fold_size,cutoff,1)\n",
    "\n",
    "#         hist = model.fit(X, T, epochs=1, batch_size=batch_size, verbose=0)\n",
    "        cb = roc_callback((X,T), (X_Val, T_Val))\n",
    "        hist = model.fit(X, T, epochs=1, batch_size=batch_size, verbose=0, \n",
    "                         callbacks=[cb])\n",
    "        \n",
    "        history_of_loss.append(hist.history['loss'])\n",
    "        histroy_of_roc.extend(cb.roc_scores)\n",
    "\n",
    "\n",
    "    # Last fold\n",
    "    last_fold_size = len(X_train) - n * fold_size\n",
    "    print('[' + str(n * fold_size) + '] to [' + str(n * fold_size + last_fold_size) + ']')   \n",
    "\n",
    "    X = construct_X(seqs_dict, X_train_shuffled, cutoff, n_gram_size, dimension, last_fold_size, n * fold_size)\n",
    "    T = np.array(T_train_shuffled[n * fold_size : n * fold_size + last_fold_size]).reshape(last_fold_size,cutoff,1)\n",
    "\n",
    "    cb = roc_callback((X,T), (X_Val, T_Val))\n",
    "    hist = model.fit(X, T, epochs=1, batch_size=batch_size, verbose=0, \n",
    "                         callbacks=[cb])\n",
    "\n",
    "    # hist = model.fit(X, T, epochs=epoch, batch_size=batch_size, verbose=1, \n",
    "    #                     callbacks=[roc_callback((X,T), (X_Val, T_Val))])\n",
    "    history_of_loss.append(hist.history['loss'])\n",
    "    histroy_of_roc.extend(cb.roc_scores)\n",
    "for i in range(epoch):\n",
    "    print ('\\n\\n*** Epoch ', i+1, ' ***')\n",
    "    training_procedure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization and Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "with open(hist_path + \"hb_20_opt\", 'w') as fp:\n",
    "        json.dump((history_of_loss, history_of_roc, history_of_f1, history_of_precision, history_of_recall), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model.save(model_path + \"hb_20_opt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_path + \"hb_20_opt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "with open(hist_path + \"bd_history_oh_20\", 'r') as fp:\n",
    "    history_of_loss, history_of_roc = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(1)\n",
    "ax_loss = fig.add_subplot(521)\n",
    "ax_loss.set_title('Loss over Iterations')\n",
    "ax_loss.plot(history_of_loss)\n",
    "\n",
    "ax_roc = fig.add_subplot(522)\n",
    "ax_roc.set_title('ROC over Iterations')\n",
    "ax_roc.plot(np.array(history_of_roc)[:,1])\n",
    "\n",
    "ax_roc = fig.add_subplot(525)\n",
    "ax_roc.set_title('F1 over Iterations')\n",
    "ax_roc.plot(np.array(history_of_f1)[:,1])\n",
    "\n",
    "ax_roc = fig.add_subplot(526)\n",
    "ax_roc.set_title('Precision over Iterations')\n",
    "ax_roc.plot(np.array(history_of_precision)[:,1])\n",
    "\n",
    "ax_roc = fig.add_subplot(529)\n",
    "ax_roc.set_title('Recall over Iterations')\n",
    "ax_roc.plot(np.array(history_of_recall)[:,1])\n",
    "\n",
    "fig.canvas.draw()\n",
    "plt.show()\n",
    "# plt.savefig(fig_path + \"bd_oh_5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "X_test, T_test = shuffleXY(X_val, T_val)\n",
    "X_t = [construct_X(seqs_dicts[0], X_test, cutoff, n_gram_size, dimensions[0], 2000, 0),\n",
    "       construct_X(seqs_dicts[1], X_test, cutoff, n_gram_size, dimensions[1], 2000, 0)]\n",
    "T_t = np.array(T_test[0: 2000]).reshape(2000,cutoff,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value :  -0.0728987  Max value :  0.033388\n",
      "Binding site index [probability]\n",
      "92 [ 0.02181328] [ True]\n",
      "94 [-0.02864159] [False]\n",
      "117 [ 0.02401555] [ True]\n",
      "Output shape :  (1, 706, 1)\n",
      "Positive predictions :  11.0\n",
      "Threshold :  0.0172560177743\n"
     ]
    }
   ],
   "source": [
    "x_t,y_t = random_sample_n_times(X_t, T_t, 1)\n",
    "output = model.predict(x_t)\n",
    "\n",
    "threshold = np.mean(output.ravel()) + 2 * np.std(output.ravel())\n",
    "print (\"Min value : \", np.min(output[0]),\" Max value : \",np.max(output[0]))\n",
    "\n",
    "print (\"Binding site index [probability]\")\n",
    "for i in range(cutoff):\n",
    "    if y_t[0][i] == 1:\n",
    "        print (i, output[0][i], output[0][i] > threshold)\n",
    "\n",
    "        \n",
    "print (\"Output shape : \", output.shape)\n",
    "print (\"Positive predictions : \", np.sum((output.ravel() > threshold) * 1.0)) \n",
    "print (\"Threshold : \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print ((output.ravel() > threshold) * 1)\n",
    "print (y_t.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.\n",
      "  0.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  1.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  1.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  1.  0.  0.  1.  0.  1.  1.  1.  0.  0.  1.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.\n",
      "  1.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  1.  0.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.\n",
      "  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  1.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.\n",
      "  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "  0.  0.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  1.\n",
      "  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "print ((output.ravel() > 0) * 1.0) # bd_oh_rp_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.\n",
      "  1.  0.  0.  0.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  1.  1.  1.  1.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  1.  1.  0.  1.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.\n",
      "  0.  0.  0.  1.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.\n",
      "  0.  1.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  1.  0.  0.  1.  1.  1.\n",
      "  0.  1.  0.  0.  1.  0.  0.  0.  1.  1.  0.  1.  1.  0.  0.  1.  0.  1.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.\n",
      "  1.  1.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  1.  1.\n",
      "  1.  1.  0.  0.  1.  0.  0.  0.  1.  1.  0.  1.  1.  0.  0.  0.  1.  1.\n",
      "  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.\n",
      "  1.  1.  0.  0.  1.  1.  1.  0.  0.  0.  0.  1.  1.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.\n",
      "  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print ((output.ravel() > 0) * 1.0) # bd_w2v_rp_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1.  0.  0.  1.\n",
      "  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  1.  0.  1.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1.\n",
      "  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  0.  1.  0.  1.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  1.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print ((output.ravel() > 0) * 1.0) # bd_oh_20"
   ]
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "metal-binding-prediction/Tian's/ProtVec_RNN.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "467px",
    "left": "964.4px",
    "right": "20px",
    "top": "214px",
    "width": "349px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
