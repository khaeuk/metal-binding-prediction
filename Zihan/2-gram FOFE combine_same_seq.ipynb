{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu,floatX=float32\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_parquet('Metal_all_20180116.snappy.parquet', engine='pyarrow')\n",
    "\n",
    "# credit: https://github.com/zygmuntz/classifying-text\n",
    "# FOFE package for encoding protein sequences with variable lengths into fixed length\n",
    "\n",
    "from fofePackage.fofe.fofe import FofeVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Data\n",
    "### 1. Eliminate X and U\n",
    "### 2. Apply a cut-off to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of sequence set before eliminating X and U: 23277\n",
      "size of sequence set after eliminating X and U: 23233\n",
      "std: 200.986 mean: 306.67\n",
      "size of sequence set before cutoff: 23233\n",
      "size of sequence set after cutoff: 22207\n",
      "maxLength after cutoff is: 707\n"
     ]
    }
   ],
   "source": [
    "proteinSet = dataset.as_matrix(columns=None)\n",
    "\n",
    "# sequence set\n",
    "sequenceSet = proteinSet[:,[4]]\n",
    "\n",
    "# fingerprint set\n",
    "fingerPrintSet = proteinSet[:,[2]]\n",
    "\n",
    "# helper methods for extracting sequence set and fingerprint set for a specific ligandID\n",
    "# return: 1: fingerPrintSet for a ligandID; 2: sequenceSet for a ligandID\n",
    "\n",
    "def extractOneLigandID(id):\n",
    "    extractedSet = proteinSet[proteinSet[:,1]==id]\n",
    "    return extractedSet[:,[2]].tolist(), extractedSet[:,[4]].tolist()\n",
    "\n",
    "fpLs_zn, seqLs_zn = extractOneLigandID(\"ZN\")\n",
    "\n",
    "# sequence list where each sequence is a list of symbols\n",
    "\n",
    "seq_ls = []\n",
    "\n",
    "for i in seqLs_zn:\n",
    "    sb_ls = []\n",
    "    for j in i[0]:\n",
    "        sb_ls.append(j)\n",
    "    seq_ls.append(sb_ls)\n",
    "    \n",
    "# eliminate X and U\n",
    "\n",
    "def eliXAndU(sqls, fpls):\n",
    "    print(\"size of sequence set before eliminating X and U: \"+str(len(sqls)))\n",
    "    sq_out = []\n",
    "    fp_out = []\n",
    "    for i in range(len(sqls)):\n",
    "        invalidSeq = False\n",
    "        for j in sqls[i]:\n",
    "            if j =='X' or j == 'U':\n",
    "                invalidSeq = True\n",
    "                break\n",
    "        if invalidSeq == False:\n",
    "            sq_out.append(sqls[i])\n",
    "            fp_out.append(fpls[i])\n",
    "            \n",
    "    print(\"size of sequence set after eliminating X and U: \"+str(len(sq_out)))\n",
    "    return sq_out, fp_out    \n",
    "\n",
    "seq_ls_eli, fp_ls_eli = eliXAndU(seq_ls, fpLs_zn)\n",
    "#seq_ls_eli, fp_ls_eli = seq_ls, fpLs_zn\n",
    "\n",
    "# make a list that stores all lengths for all sequence to make a cut-off\n",
    "\n",
    "length_ls = []\n",
    "\n",
    "for i in seq_ls_eli:\n",
    "    curLen = 0\n",
    "    for j in i:\n",
    "        curLen +=1\n",
    "    \n",
    "    length_ls.append(curLen)\n",
    "    \n",
    "# helper function to get a sub list of current sequence list and foot print list\n",
    "\n",
    "def getSubList(sqls, fpls, cutoff):\n",
    "    sq_out = []\n",
    "    fp_out = []\n",
    "    for i in range(len(sqls)):\n",
    "        if len(sqls[i]) <= cutoff:\n",
    "            sq_out.append(sqls[i])\n",
    "            fp_out.append(fpls[i])\n",
    "    \n",
    "    return sq_out, fp_out\n",
    "\n",
    "\n",
    "# sort length list to do a cut-off\n",
    "length_ls.sort()\n",
    "length_ls = np.array(length_ls)\n",
    "\n",
    "stdLength = np.std(length_ls, dtype=np.float32)\n",
    "meanLength = np.mean(length_ls, dtype=np.float32)\n",
    "\n",
    "print(\"std: \"+str(stdLength)+\" mean: \"+str(meanLength))\n",
    "\n",
    "\n",
    "# do cut-off\n",
    "seq_ls_cut, fp_ls_cut = getSubList(seq_ls_eli, fp_ls_eli, meanLength+2*stdLength)\n",
    "\n",
    "\n",
    "\n",
    "# don't do cut-off\n",
    "# seq_ls_cut = seq_ls\n",
    "# fp_ls_cut = fpLs_zn\n",
    "\n",
    "print(\"size of sequence set before cutoff: \"+str(len(seq_ls_eli)))\n",
    "print(\"size of sequence set after cutoff: \"+str(len(seq_ls_cut)))\n",
    "\n",
    "# find the max length of sequences after cut off\n",
    "maxLength = 0\n",
    "\n",
    "for i in seq_ls_cut:\n",
    "    curLen = 0\n",
    "    for j in i:\n",
    "        curLen +=1\n",
    "    \n",
    "    if curLen>maxLength:\n",
    "        maxLength = curLen\n",
    "print(\"maxLength after cutoff is: \" + str(maxLength))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C and H takes: 0.3700922303760601\n",
      "C and H in zinc binding sequences takes: 0.8088663790408304\n"
     ]
    }
   ],
   "source": [
    "# C & H take 37% in all sequences and 81% in sequences with zinc binding\n",
    "C_H_count = 0\n",
    "total_binding_sites = 0\n",
    "\n",
    "for i in range(len(sequenceSet)):\n",
    "    for j in fingerPrintSet[i][0]:\n",
    "        if sequenceSet[i][0][j]== \"C\" or sequenceSet[i][0][j] == \"H\":\n",
    "            C_H_count+=1\n",
    "        total_binding_sites+=1\n",
    "\n",
    "print(\"C and H takes: \"+str(C_H_count/total_binding_sites))\n",
    "        \n",
    "\n",
    "C_H_count_zn = 0\n",
    "total_binding_sites_zn = 0\n",
    "\n",
    "for i in range(len(seq_ls_cut)):\n",
    "    for j in fp_ls_cut[i][0]:\n",
    "        if seq_ls_cut[i][j] == \"C\" or seq_ls_cut[i][j] == \"H\":\n",
    "            C_H_count_zn+=1\n",
    "        total_binding_sites_zn+=1\n",
    "\n",
    "print(\"C and H in zinc binding sequences takes: \"+str(C_H_count_zn/total_binding_sites_zn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_seq_ls = []\n",
    "\n",
    "for i in seq_ls_cut:\n",
    "    full_seq_ls.append(\"\".join(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare parameters for FOFE encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pre-process seq_ls_cut to make it a 2-gram dataset: ABC -> AB, BC\n",
    "two_gram_ls = []\n",
    "\n",
    "for i in seq_ls_cut:\n",
    "    cur_two_gram = []\n",
    "    for j in range(len(i)-1):\n",
    "        cur_two_gram.append(i[j]+i[j+1])\n",
    "    two_gram_ls.append(cur_two_gram)\n",
    "\n",
    "seq_ls_cut = two_gram_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a dict to find out what 2-gram types the sequences have\n",
    "# Ignore X and U symbols\n",
    "symbolDict = {}\n",
    "for i in seq_ls_cut:\n",
    "    for j in i:\n",
    "            symbolDict[j] = 1\n",
    "\n",
    "# vocabulary: a dictionary mapping symbols (amino acid) to indices\n",
    "\n",
    "# raw_doc: list of all symbols\n",
    "raw_doc = []\n",
    "\n",
    "for key, _ in symbolDict.items():\n",
    "    raw_doc.append(key)\n",
    "\n",
    "    \n",
    "# # # Marker B\n",
    "# raw_doc.append('B')\n",
    "    \n",
    "# create the vocabulary dictionary\n",
    "vocab_dict = {}\n",
    "count = 0\n",
    "for sb in raw_doc:\n",
    "    vocab_dict[sb] = count\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # insert marker symbol \"B\" to the beginning of all sequences\n",
    "\n",
    "# for i in range(len(seq_ls_cut)):\n",
    "#     seq_ls_cut[i] = ['B']+seq_ls_cut[i]\n",
    "    \n",
    "# # increase all finger print data by 1\n",
    "\n",
    "# fp_ls_inc_by_1 = []\n",
    "\n",
    "# for i in range(len(fp_ls_cut)):\n",
    "#     cur_fp_ls = np.array([])\n",
    "#     for j in range(len(fp_ls_cut[i][0])):\n",
    "#         cur_fp_ls = np.append(cur_fp_ls, np.array([fp_ls_cut[i][0][j]+1])).astype(int)\n",
    "#     fp_ls_inc_by_1.append([cur_fp_ls])\n",
    "\n",
    "# fp_ls_cut = fp_ls_inc_by_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# BUILD of a simple CNN architecture\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "np.random.seed(2017) \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Convolution1D, MaxPooling1D, AveragePooling2D\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout, Reshape, Embedding, Input\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to find the ranking of finger print points in prediction\n",
    "def getF1(sequence_index, label_set, pred, factor):\n",
    "    \n",
    "    prob_mean = np.mean(pred[sequence_index])\n",
    "    prob_std = np.std(pred[sequence_index])\n",
    "    \n",
    "    prob_th = prob_mean+factor*prob_std\n",
    "    \n",
    "    cur_fp_ls = label_set[sequence_index][0]\n",
    "    truePositive = 0\n",
    "    falseNegative = 0\n",
    "    for i in range(len(cur_fp_ls)):\n",
    "        cur_index = cur_fp_ls[i]\n",
    "        if pred[sequence_index][cur_index] >= prob_th:\n",
    "            truePositive += 1\n",
    "        else:\n",
    "            falseNegative += 1\n",
    "    \n",
    "    Positive = 0\n",
    "    Negative = 0\n",
    "    for i in range(len(pred[sequence_index])):\n",
    "        if pred[sequence_index][i] >= prob_th:\n",
    "            Positive += 1\n",
    "        else:\n",
    "            Negative += 1\n",
    "                \n",
    "    return truePositive, falseNegative, Positive, Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getF1_new(sequence_index, label_set, pred, factor):\n",
    "    \n",
    "    prob_mean = np.mean(pred[sequence_index])\n",
    "    prob_std = np.std(pred[sequence_index]) \n",
    "    prob_th = prob_mean+factor*prob_std\n",
    "    cur_fp_ls = label_set[sequence_index]\n",
    "    truePositive = 0\n",
    "    falseNegative = 0\n",
    "    falsePositive = 0\n",
    "    for i in range(len(cur_fp_ls)):\n",
    "        if(cur_fp_ls[i] == 1):\n",
    "            if pred[sequence_index][i] >= prob_th:\n",
    "                truePositive += 1\n",
    "            else:\n",
    "                falseNegative += 1\n",
    "        else:\n",
    "            if pred[sequence_index][i] >= prob_th and cur_fp_ls[i] == 0:\n",
    "                falsePositive += 1\n",
    "    \n",
    "    Positive = 0\n",
    "    Negative = 0\n",
    "    for i in range(len(pred[sequence_index])):\n",
    "        if pred[sequence_index][i] >= prob_th:\n",
    "            Positive += 1\n",
    "        else:\n",
    "            Negative += 1\n",
    "            \n",
    "    return truePositive, falseNegative, Positive, Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to find the ranking of finger print points in prediction\n",
    "def getRanking(sequence_index, label_set, pred):\n",
    "    # build a list of tuples based on the prediction list\n",
    "    tuple_ls = []\n",
    "    for i in range(len(pred[sequence_index])):\n",
    "        tuple_ls.append((i, pred[sequence_index][i]))\n",
    "    \n",
    "    tuple_ls = sorted(tuple_ls, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    output_ls = []\n",
    "    cur_fp_ls = label_set[sequence_index][0]\n",
    "    \n",
    "    #print(cur_fp_ls)\n",
    "    for i in range(len(cur_fp_ls)):\n",
    "        \n",
    "        cur_index = cur_fp_ls[i]\n",
    "        for j in range(len(tuple_ls)):\n",
    "            if tuple_ls[j][0]==cur_index:\n",
    "                output_ls.append(j+1)\n",
    "                #print(\"Ranking for site \"+str(cur_index)+\" is: \"+str(j+1))\n",
    "                \n",
    "    return output_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For hybrid model only\n",
    "def random_sample_n_times(X, Y, size):\n",
    "    X1 = X\n",
    "    n = X1.shape[0]\n",
    "    _X1, _Y = [], []\n",
    "    for i in range(size):\n",
    "        r = np.random.randint(n)\n",
    "        _X1.append(X1[r].tolist())\n",
    "        _Y.append(Y[r].tolist())\n",
    "        \n",
    "    _X = np.asarray(_X1)\n",
    "    _Y = np.asarray(_Y)\n",
    "    \n",
    "    return _X, _Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class custom_callback(Callback):\n",
    "    def __init__(self,training_data,validation_data, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "        \n",
    "        self.roc_scores = []\n",
    "        self.f1_scores = []\n",
    "        self.precision_scores = []\n",
    "        self.recall_scores = []\n",
    "        \n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.roc_scores = []\n",
    "        self.f1_scores = []\n",
    "        self.precision_scores = []\n",
    "        self.recall_scores = []\n",
    "        return\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = random_sample_n_times(self.x, self.y, self.sample_size)\n",
    "        x_val, y_val = random_sample_n_times(self.x_val, self.y_val, self.sample_size)\n",
    "\n",
    "        y_pred = self.model.predict(x)\n",
    "        y_pred_val = self.model.predict(x_val)\n",
    "    \n",
    "        tp_sum = 0\n",
    "        fn_sum = 0\n",
    "        p_sum = 0\n",
    "        n_sum = 0\n",
    "        \n",
    "        # calculate f1 score in the batch from validation set\n",
    "        for i in range(self.sample_size):\n",
    "            tp,fn,p,n = getF1_new(i, y_val, y_pred_val,4)\n",
    "            tp_sum = tp_sum + tp\n",
    "            fn_sum = fn_sum + fn\n",
    "            p_sum = p_sum + p\n",
    "            n_sum = n_sum + n\n",
    "\n",
    "        precision = tp_sum/p_sum\n",
    "        recall = tp_sum/(tp_sum+fn_sum) \n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "        self.f1_scores.append((len(self.f1_scores),f1))\n",
    "        \n",
    "        return\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look at all sequences and create key value pair for duplicate sequences and respective fp\n",
    "seq_dict = {}\n",
    "\n",
    "for i in range(len(seqLs_zn)):\n",
    "    if seqLs_zn[i][0] in seq_dict:\n",
    "        for j in range(fpLs_zn[i][0].size):\n",
    "            seq_dict[seqLs_zn[i][0]][fpLs_zn[i][0][j]] = 1\n",
    "    else:\n",
    "        seq_dict[seqLs_zn[i][0]] = {}\n",
    "        for j in range(fpLs_zn[i][0].size):\n",
    "            seq_dict[seqLs_zn[i][0]][fpLs_zn[i][0][j]] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{188: 1, 190: 1, 234: 1, 240: 1, 276: 1, 306: 1, 308: 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_dict[full_seq_ls[int(len(full_seq_ls)*3/4)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildFeatureAndTrain(alpha,factor):\n",
    "\n",
    "    print(\"alpha = \"+str(alpha))\n",
    "    #alpha = 1 - 1e-3\n",
    "    #alpha = 0.4\n",
    "\n",
    "    fofe = FofeVectorizer(alpha)\n",
    "\n",
    "    # naive transform: slow and memory hungry, but is readable\n",
    "    features = fofe.naive_transform(seq_ls_cut, vocab_dict)\n",
    "    labels = fp_ls_cut[:]\n",
    "\n",
    "    # get reverse of sequences\n",
    "    seq_ls_cut_rev = []\n",
    "    for i in seq_ls_cut:\n",
    "        seq_ls_cut_rev.append(list(reversed(i)))\n",
    "    features_rev = fofe.naive_transform(seq_ls_cut_rev, vocab_dict)\n",
    "\n",
    "    # normalize feature matrix by subtracting each value by mean/std\n",
    "    for i in range(len(features)):\n",
    "        tmp_ls = np.array(features[i])\n",
    "        tmp_std = np.std(tmp_ls, dtype=np.float32)\n",
    "        tmp_mean = np.mean(tmp_ls, dtype=np.float32)\n",
    "        for j in range(len(tmp_ls)):\n",
    "            features[i][j]=tmp_ls[j]-tmp_mean/tmp_std\n",
    "\n",
    "    # normalize feature matrix by subtracting each value by mean/std\n",
    "    for i in range(len(features_rev)):\n",
    "        tmp_ls = np.array(features_rev[i])\n",
    "        tmp_std = np.std(tmp_ls, dtype=np.float32)\n",
    "        tmp_mean = np.mean(tmp_ls, dtype=np.float32)\n",
    "        for j in range(len(tmp_ls)):\n",
    "            features_rev[i][j]=tmp_ls[j]-tmp_mean/tmp_std\n",
    "\n",
    "    # Combine features and features_rev\n",
    "    features_new = []\n",
    "    for i in range(len(features)):\n",
    "        features_new.append(np.concatenate((features[i],features_rev[i])))\n",
    "\n",
    "    features = features_new\n",
    "    #print(features_new[0])\n",
    "\n",
    "\n",
    "    train_features = features[:int(len(features)*3/4)]\n",
    "    train_labels = labels[:int(len(labels)*3/4)]\n",
    "    \n",
    "    vali_features = features[int(len(features)*3/4):int(len(features)*7/8)]\n",
    "    vali_labels = labels[int(len(labels)*3/4):int(len(labels)*7/8)]\n",
    "\n",
    "    test_features = features[int(len(features)*7/8):]\n",
    "    test_labels = labels[int(len(labels)*7/8):]\n",
    "\n",
    "    train_seq_ls_cut = seq_ls_cut[:int(len(seq_ls_cut)*3/4)]\n",
    "    vali_seq_ls_cut = seq_ls_cut[int(len(seq_ls_cut)*3/4):int(len(seq_ls_cut)*7/8)]\n",
    "    test_seq_ls_cut = seq_ls_cut[int(len(seq_ls_cut)*7/8):]\n",
    "\n",
    "    train_fp_ls_cut = fp_ls_cut[:int(len(fp_ls_cut)*3/4)]\n",
    "    vali_fp_ls_cut = fp_ls_cut[int(len(fp_ls_cut)*3/4):int(len(fp_ls_cut)*7/8)]\n",
    "    test_fp_ls_cut = fp_ls_cut[int(len(fp_ls_cut)*7/8):] \n",
    "    \n",
    "    train_full_seq_ls = full_seq_ls[:int(len(full_seq_ls)*3/4)]\n",
    "    vali_full_seq_ls = full_seq_ls[int(len(full_seq_ls)*3/4):int(len(full_seq_ls)*7/8)]\n",
    "    test_full_seq_ls = full_seq_ls[int(len(full_seq_ls)*7/8):]\n",
    "\n",
    "    train_features = np.array(train_features)\n",
    "    vali_features = np.array(vali_features)\n",
    "    test_features = np.array(test_features)\n",
    "    \n",
    "    # get test_labels and vali_labels combined for duplicate sequences\n",
    "    for i in range(len(vali_seq_ls_cut)):\n",
    "        for cur_fp in seq_dict[vali_full_seq_ls[i]]:\n",
    "            if cur_fp not in vali_labels[i][0]:\n",
    "                vali_labels[i][0] = np.append(vali_labels[i][0], [cur_fp]) \n",
    "        \n",
    "        \n",
    "    for i in range(len(test_seq_ls_cut)):\n",
    "        for cur_fp in seq_dict[test_full_seq_ls[i]]:\n",
    "            if cur_fp not in test_labels[i][0]:\n",
    "                test_labels[i][0] = np.append(test_labels[i][0], [cur_fp])       \n",
    "\n",
    "    #prep of labels\n",
    "    #create zero matrix of all zeros with maxLength columns\n",
    "    train_l = np.zeros((len(train_labels),maxLength))\n",
    "    vali_l = np.zeros((len(vali_labels),maxLength))\n",
    "    test_l = np.zeros((len(test_labels),maxLength))\n",
    "\n",
    "    for i in range(len(train_labels)):\n",
    "        for j in train_labels[i][0]:\n",
    "            train_l[i][j] = 1\n",
    "\n",
    "    for i in range(len(vali_labels)):\n",
    "        for j in vali_labels[i][0]:\n",
    "            vali_l[i][j] = 1\n",
    "            \n",
    "    for i in range(len(test_labels)):\n",
    "        for j in test_labels[i][0]:\n",
    "            test_l[i][j] = 1    \n",
    "            \n",
    "    print(vali_l[0])\n",
    "            \n",
    "\n",
    "    train_l = np.array(train_l)\n",
    "    vali_l = np.array(vali_l)\n",
    "    test_l = np.array(test_l)\n",
    "    \n",
    "    #The MODEL\n",
    "    #3 parallel conv2d layers\n",
    "    \n",
    "    fofe_input = Input(shape=(800,), dtype='float32', name='fofe_input')\n",
    "    fofe_input_reshape = Reshape((1,800,1), input_shape=(800,))(fofe_input)\n",
    "    conv2d_3 = Convolution2D(3, 1, 6, border_mode='same')(fofe_input_reshape)\n",
    "    conv2d_5 = Convolution2D(5, 1, 4, border_mode='same')(fofe_input_reshape)\n",
    "    conv2d_7 = Convolution2D(7, 1, 2, border_mode='same')(fofe_input_reshape)\n",
    "    \n",
    "    x = keras.layers.concatenate([conv2d_3,conv2d_5,conv2d_7])\n",
    "    x = Activation('relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(maxLength, activation='relu')(x)\n",
    "    fofe_output = Dense(maxLength, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=fofe_input, outputs=fofe_output)\n",
    "    \n",
    "    # end of the MODEL\n",
    "\n",
    "    sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # callbacks loss history\n",
    "\n",
    "    cb = custom_callback((train_features,train_l), (vali_features, vali_l), 128)\n",
    "    start = time.time()\n",
    "    \n",
    "    model.fit(train_features, train_l, batch_size=128,epochs=20, validation_data=(vali_features, vali_l),\n",
    "              verbose=1,callbacks=[cb])\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"Model took %0.2f seconds to train\"%(end - start))\n",
    "    \n",
    "    print(cb.f1_scores)\n",
    "    \n",
    "    fig1 = plt.figure()\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    #ax1.scatter(rank_x,rank_y,s=4)\n",
    "    ax1.scatter(*zip(*cb.f1_scores))\n",
    "    #ax1.plot(cb.f1_scores)\n",
    "    ax1.set_ylabel('f1')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    \n",
    "    prediction = model.predict(test_features)\n",
    "    \n",
    "    rank_x = []\n",
    "    rank_y = []\n",
    "\n",
    "    hit = 0\n",
    "    total = 0\n",
    "    \n",
    "    tp_sum = 0\n",
    "    fn_sum = 0\n",
    "    p_sum = 0\n",
    "    n_sum = 0\n",
    "    \n",
    "    for i in range(len(test_labels)):\n",
    "        tp,fn,p,n = getF1(i, test_labels, prediction,factor)\n",
    "        tp_sum = tp_sum + tp\n",
    "        fn_sum = fn_sum + fn\n",
    "        p_sum = p_sum + p\n",
    "        n_sum = n_sum + n\n",
    "\n",
    "    \n",
    "    precision = tp_sum/p_sum\n",
    "    recall = tp_sum/(tp_sum+fn_sum) \n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    print(\"precision = \"+str(precision))\n",
    "    print(\"recall = \"+str(recall))\n",
    "    print(\"f1 = \"+str(f1))\n",
    "    return f1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.95\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:111: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(3, (1, 6), padding=\"same\")`\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:112: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(5, (1, 4), padding=\"same\")`\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(7, (1, 2), padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16655 samples, validate on 2776 samples\n",
      "Epoch 1/20\n",
      "16655/16655 [==============================] - 14s 868us/step - loss: 15.7628 - acc: 0.1192 - val_loss: 23.8208 - val_acc: 0.1470\n",
      "Epoch 2/20\n",
      "16655/16655 [==============================] - 14s 844us/step - loss: 10.8371 - acc: 0.1862 - val_loss: 21.5001 - val_acc: 0.2068\n",
      "Epoch 3/20\n",
      "16655/16655 [==============================] - 14s 851us/step - loss: 9.3677 - acc: 0.2100 - val_loss: 20.1213 - val_acc: 0.1574\n",
      "Epoch 4/20\n",
      "16655/16655 [==============================] - 14s 842us/step - loss: 8.6562 - acc: 0.2107 - val_loss: 19.4015 - val_acc: 0.1837\n",
      "Epoch 5/20\n",
      "16655/16655 [==============================] - 14s 844us/step - loss: 8.1755 - acc: 0.2132 - val_loss: 19.2634 - val_acc: 0.1783\n",
      "Epoch 6/20\n",
      "16655/16655 [==============================] - 14s 848us/step - loss: 8.0396 - acc: 0.2199 - val_loss: 19.0077 - val_acc: 0.2006\n",
      "Epoch 7/20\n",
      "16655/16655 [==============================] - 15s 871us/step - loss: 7.7965 - acc: 0.2117 - val_loss: 18.6801 - val_acc: 0.1664\n",
      "Epoch 8/20\n",
      "16655/16655 [==============================] - 14s 838us/step - loss: 7.5737 - acc: 0.2210 - val_loss: 18.3565 - val_acc: 0.2439\n",
      "Epoch 9/20\n",
      "16655/16655 [==============================] - 14s 832us/step - loss: 7.3810 - acc: 0.2219 - val_loss: 18.2636 - val_acc: 0.1765\n",
      "Epoch 10/20\n",
      "16655/16655 [==============================] - 14s 842us/step - loss: 7.2870 - acc: 0.2232 - val_loss: 18.0735 - val_acc: 0.1920\n",
      "Epoch 11/20\n",
      "16655/16655 [==============================] - 14s 840us/step - loss: 7.1605 - acc: 0.2226 - val_loss: 18.2591 - val_acc: 0.1635\n",
      "Epoch 12/20\n",
      "16655/16655 [==============================] - 14s 831us/step - loss: 7.1379 - acc: 0.2219 - val_loss: 18.0338 - val_acc: 0.1974\n",
      "Epoch 13/20\n",
      "16655/16655 [==============================] - 14s 831us/step - loss: 7.0341 - acc: 0.2241 - val_loss: 18.2858 - val_acc: 0.1646\n",
      "Epoch 14/20\n",
      "16655/16655 [==============================] - 14s 839us/step - loss: 7.0321 - acc: 0.2277 - val_loss: 17.9351 - val_acc: 0.1938\n",
      "Epoch 15/20\n",
      "16655/16655 [==============================] - 14s 840us/step - loss: 6.9202 - acc: 0.2270 - val_loss: 18.0050 - val_acc: 0.1704\n",
      "Epoch 16/20\n",
      "16655/16655 [==============================] - 14s 842us/step - loss: 7.0128 - acc: 0.2328 - val_loss: 18.0687 - val_acc: 0.1643\n",
      "Epoch 17/20\n",
      "16655/16655 [==============================] - 14s 840us/step - loss: 6.9312 - acc: 0.2235 - val_loss: 17.7938 - val_acc: 0.1891\n",
      "Epoch 18/20\n",
      "16655/16655 [==============================] - 14s 846us/step - loss: 6.8759 - acc: 0.2217 - val_loss: 17.9225 - val_acc: 0.1888\n",
      "Epoch 19/20\n",
      "16655/16655 [==============================] - 14s 846us/step - loss: 6.8209 - acc: 0.2205 - val_loss: 17.7974 - val_acc: 0.1942\n",
      "Epoch 20/20\n",
      "16655/16655 [==============================] - 14s 836us/step - loss: 6.7713 - acc: 0.2293 - val_loss: 17.9342 - val_acc: 0.2046\n",
      "Model took 281.42 seconds to train\n",
      "[(0, 0.43286573146292584), (1, 0.6182760855476346), (2, 0.7054698457223002), (3, 0.6088050314465409), (4, 0.7356797791580401), (5, 0.7235955056179776), (6, 0.7487179487179488), (7, 0.763758389261745), (8, 0.7812274368231046), (9, 0.7170311464546058), (10, 0.7369175627240142), (11, 0.7726063829787234), (12, 0.7601156069364161), (13, 0.8014285714285714), (14, 0.7812061711079944), (15, 0.8610169491525423), (16, 0.8049901510177282), (17, 0.8172484599589322), (18, 0.7548387096774193), (19, 0.8286304198210598)]\n",
      "precision = 0.8298045130227971\n",
      "recall = 0.8766930195501624\n",
      "f1 = 0.8526046012635594\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3pRkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x237820b7b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# get ranking plot for test set\n",
    "\n",
    "#alpha_to_test = [1 - 5e-3, 1 - 1e-2, 1 - 5e-2, 1 - 1e-1, 1 - 4e-2]\n",
    "alpha_to_test = [0.95]\n",
    "factor_to_test = [2]\n",
    "\n",
    "f1_ls = []\n",
    "\n",
    "for t in factor_to_test:\n",
    "    f1_ls.append(buildFeatureAndTrain(0.95,t))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'factor multiplies std')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAv8PmvvM1JVFvOIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x237e12aab00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(111)\n",
    "#ax1.scatter(rank_x,rank_y,s=4)\n",
    "ax1.scatter(factor_to_test,f1_ls)\n",
    "ax1.plot(factor_to_test,f1_ls)\n",
    "ax1.set_ylabel('f 1')\n",
    "ax1.set_xlabel('factor multiplies std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_ls = []\n",
    "tmp_count = 0\n",
    "for i in range(maxLength):\n",
    "    index_ls.append(tmp_count)\n",
    "    tmp_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(111)\n",
    "ax2.scatter(index_ls,prediction[0],s=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.histogram(np.array(prediction[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2*std + mean\n",
    "precision = 0.8298045130227971\n",
    "recall = 0.8766930195501624\n",
    "f1 = 0.8526046012635594"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
