{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T18:38:07.710619Z",
     "start_time": "2018-06-06T18:38:07.676898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T18:38:09.207350Z",
     "start_time": "2018-06-06T18:38:09.198424Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = 'PART2'\n",
    "metal = 'ZN'\n",
    "model_name = metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T18:38:11.943318Z",
     "start_time": "2018-06-06T18:38:10.062789Z"
    },
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing global variables... Done\n",
      "  Filepath set to ./logs/\n",
      "Importing modules... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Reading data from disk... Done [ZN]\n",
      "Loading dictionaries... Done\n",
      "Performing cross validation split... Done\n",
      "  Ratio : 0.9\n",
      "  Train_range : 0 - 17279\n",
      "  Val_range : 17280 - 19199\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print (\"Initializing global variables...\", end=' ')\n",
    "sys.stdout.flush()\n",
    "\n",
    "# shared result file\n",
    "output_file = './logs/results.txt'\n",
    "hist_path = model_path = fig_path = './logs/'\n",
    "dict_path = './dictionaries/'\n",
    "\n",
    "print (\"Done\")\n",
    "# print (\"  Filepath set to [\" + user + \"]'s directory\")\n",
    "print (\"  Filepath set to ./logs/\")\n",
    "\n",
    "\n",
    "##################################################\n",
    "\n",
    "print (\"Importing modules...\", end=' ')\n",
    "import modules\n",
    "print (\"Done\")\n",
    "\n",
    "##################################################\n",
    "\n",
    "print (\"Reading data from disk...\", end=' ')\n",
    "sys.stdout.flush()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('./datasets/Metal_all_20180601_predicted.parquet')\n",
    "\n",
    "\n",
    "# Extract the metal\n",
    "df_metal = df.loc[df['metalPrediction'] == metal]\n",
    "# df_zn_single = df_zn.loc[df_zn['interactingChains'] == 1]\n",
    "\n",
    "seqs = np.array(df_metal.sequence)\n",
    "target = np.array(df_metal.fingerprint)\n",
    "cluster_numbers = np.array(df_metal.clusterNumber90)\n",
    "\n",
    "# del df,df_zn,df_zn_single\n",
    "\n",
    "\n",
    "print (\"Done [\" + metal + \"]\")\n",
    "\n",
    "##################################################\n",
    "\n",
    "# print (\"Removing 'U','X', NaN...\", end=' ')\n",
    "# sys.stdout.flush()\n",
    "# # Remove seqs containing 'U' and 'X'\n",
    "# rows_to_delete = []\n",
    "# for i in range(seqs.shape[0]):\n",
    "#     if 'X' in seqs[i] or 'U' in seqs[i] or np.isnan(cluster_numbers[i]):\n",
    "#         rows_to_delete.append(i) \n",
    "#         print (\"Removing\", i)\n",
    "\n",
    "\n",
    "\n",
    "# seqs = np.delete(seqs, rows_to_delete, 0)\n",
    "# target = np.delete(target, rows_to_delete)\n",
    "# cluster_numbers = np.delete(cluster_numbers, rows_to_delete)\n",
    "# print (\"Done\")\n",
    "\n",
    "##################################################\n",
    "\n",
    "print (\"Loading dictionaries...\", end=' ')\n",
    "sys.stdout.flush()\n",
    "import json\n",
    "\n",
    "# # ProtVec\n",
    "# seqs_dict_w2v = {}\n",
    "# with open(dict_path + \"seq_n_gram_to_vec_dict_w_UX\", 'r') as fp:\n",
    "#         seqs_dict_w2v = json.load(fp)\n",
    "\n",
    "# # One-hot\n",
    "# seqs_dict_onehot = {}\n",
    "# with open(dict_path + \"seqs_dict_onehot\", 'r') as fp:\n",
    "#         seqs_dict_onehot = json.load(fp)\n",
    "\n",
    "# FOFE\n",
    "vocab_dic_fofe = {}\n",
    "with open(dict_path + \"vocab_dict_fofe\", 'r') as fp:\n",
    "        vocab_dic_fofe = json.load(fp)\n",
    "            \n",
    "            \n",
    "# # property\n",
    "# # blosum62\n",
    "# from proteinSequenceEncoder import property_encoder, blosum62_encoder \n",
    "# AMINO_ACIDS21 = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', \\\n",
    "#                  'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'X', 'Y']\n",
    "# seqs_property = {}\n",
    "# seqs_blosum62 = {}\n",
    "# seqs_onehot_blosum = {}\n",
    "\n",
    "# for aa in AMINO_ACIDS21:\n",
    "#     seqs_property[aa] = property_encoder(aa)\n",
    "#     seqs_blosum62[aa] = blosum62_encoder(aa)\n",
    "#     if aa != 'X' and aa !='U':\n",
    "#         seqs_onehot_blosum[aa] = seqs_dict_onehot[aa] + blosum62_encoder(aa)[0]\n",
    "\n",
    "\n",
    "print (\"Done\")\n",
    "\n",
    "##################################################\n",
    "\n",
    "print (\"Performing cross validation split...\", end=' ')\n",
    "ratio = 0.9\n",
    "split = int(ratio*len(seqs))\n",
    "train_seqs, val_seqs = seqs[:split], seqs[split:]\n",
    "train_label, val_label = target[:split], target[split:]\n",
    "train_cluster, val_cluster = cluster_numbers[:split], cluster_numbers[split:]\n",
    "print (\"Done\")\n",
    "print (\"  Ratio :\", ratio)\n",
    "print (\"  Train_range :\", 0, \"-\", split-1)\n",
    "print (\"  Val_range :\", split, \"-\", len(seqs)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T18:38:12.904069Z",
     "start_time": "2018-06-06T18:38:12.284537Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_args = {'sequences': train_seqs,\n",
    "              'labels': train_label,\n",
    "              'translator': vocab_dic_fofe}\n",
    "val_args = {'sequences': val_seqs,\n",
    "            'labels': val_label,\n",
    "            'translator': vocab_dic_fofe}\n",
    "common_args = {'batch_size': 100,\n",
    "               'input_shape': (800,),\n",
    "               'label_shape': (706, ),\n",
    "               'shuffle': True}\n",
    "\n",
    "train_gen = modules.FOFEGenerator(**train_args, **common_args)\n",
    "val_gen = modules.FOFEGenerator(**val_args, **common_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T18:38:13.457604Z",
     "start_time": "2018-06-06T18:38:13.245290Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tian\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2, (1, 3), padding=\"same\")`\n",
      "C:\\Users\\Tian\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2, (1, 5), padding=\"same\")`\n",
      "C:\\Users\\Tian\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2, (1, 7), padding=\"same\")`\n"
     ]
    }
   ],
   "source": [
    "# ProtVec:100, One-hot:20, blosum62:20, property:7\n",
    "dimension = 800\n",
    "cutoff = 706\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "np.random.seed(2017) \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Convolution1D, MaxPooling1D, AveragePooling2D\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout, Reshape, Embedding, Input\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "# Visualization\n",
    "from keras.utils import plot_model\n",
    "\n",
    "input_shape = (dimension,)\n",
    "\n",
    "input_0 = Input(shape=input_shape, dtype='float32')\n",
    "input_0_reshape = Reshape((1,dimension,1), input_shape=(dimension,))(input_0)\n",
    "conv2d_3 = Convolution2D(2, 1, 3, border_mode='same')(input_0_reshape)\n",
    "conv2d_5 = Convolution2D(2, 1, 5, border_mode='same')(input_0_reshape)\n",
    "conv2d_7 = Convolution2D(2, 1, 7, border_mode='same')(input_0_reshape)\n",
    "\n",
    "x = keras.layers.concatenate([conv2d_3,conv2d_5,conv2d_7])\n",
    "x = Activation('relu')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(cutoff, activation='relu')(x)\n",
    "output_0 = Dense(cutoff, activation='softmax')(x)\n",
    "#output_0_reshape = Reshape((cutoff,1), input_shape=(cutoff,))(output_0)\n",
    "\n",
    "#model = Model(inputs=input_0, outputs=output_0_reshape)\n",
    "model = Model(inputs=input_0, outputs=output_0)                              \n",
    "# end of the MODEL\n",
    "\n",
    "sgd = SGD(lr = 0.01, momentum = 0.9, decay = 0, nesterov = False)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=blue>Threshold: mean+2.33*std</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T18:38:13.831561Z",
     "start_time": "2018-06-06T18:38:13.825609Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "factor = 2.33\n",
    "def threshold_func(y_in):\n",
    "    factor = 2.33\n",
    "    y_out = np.zeros_like(y_in)\n",
    "    for i in range(y_in.shape[0]):\n",
    "        th= np.mean(y_in[i]) + factor * np.std(y_in[i])\n",
    "        y_out[i] = (y_in[i] > th)\n",
    "    return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=blue>Metric: F1 score</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T18:38:15.837687Z",
     "start_time": "2018-06-06T18:38:14.879232Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback initialized.\n",
      "Assigning validation generator... Done\n",
      "Matching input shape... Done\n",
      "Matching output shape... Done\n",
      "Trainer initialized.\n"
     ]
    }
   ],
   "source": [
    "cb = modules.F1_history(threshold_func)\n",
    "\n",
    "model_args = {'model': model, \n",
    "              'generators': [train_gen, val_gen], \n",
    "              'callbacks': [cb], \n",
    "              'post_train_args': {'user': user, \n",
    "                                  'model': model_name, \n",
    "                                  'result': output_file, \n",
    "                                  'fig_path': fig_path, \n",
    "                                  'optimizer': str(type(model.optimizer)).replace('<class \\'keras.optimizers.', '').replace('\\'>', ''), \n",
    "                                  'optimizer_config' : model.optimizer.get_config(), \n",
    "                                  'loss': model.loss, \n",
    "                                  'factor': factor}}\n",
    "\n",
    "trainer = modules.Trainer(**model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-06T18:38:19.359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "import warnings; \n",
    "warnings.simplefilter('ignore')\n",
    "trainer.start(epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-06T18:38:21.559Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(metal + \".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(metal + \".h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# later...\n",
    "# load json and create model\n",
    "json_file = open(metal + \".json\", \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(metal + \".h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
